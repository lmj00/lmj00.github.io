---
title: "Apache Kafka"
layout: post
category: kafka
---


### Apache Kafka
> 빠르고 확장 가능한 작업을 위해 데이터 피드의 분산 스트리밍, 파이프 라이닝 및 재생을 위한 실시간 스트리밍 데이터를 처리하기 위한 목적으로 설계된 오픈 소스 분산형 게시-구독 메시징 플랫폼입니다.

> Kafka는 서버 클러스터 내에서 데이터 스트림을 레코드로 유지하는 방식으로 작동하는 브로커 기반 솔루션입니다. 

> Kafka 서버는 여러 데이터 센터에 분산되어 있을 수 있으며 여러 서버 인스턴스에 걸쳐 레코드 스트림(메시지)을 토픽으로 저장하여 데이터 지속성을 제공할 수 있습니다. 토픽은 레코드 또는 메시지를 키, 값 및 타임스탬프로 구성된 일련의 튜플, 변경 불가능한 Python 객체 시퀀스로 저장합니다.

> 레코드는 변경 불가능한 커밋 로그라고 합니다. 변경 불가능하다고 하는 것은 레코드를 추가할 수는 있지만 달리 변경할 수는 없기 때문입니다. 여기에서 로그를 구독(데이터에 액세스)할 수 있으며, 개수 제한 없이 여러 스트리밍 실시간 애플리케이션과 다른 시스템에서 로그에 게시(데이터 추가)할 수도 있습니다.


### kafka 등장 배경
> 전통적인 [엔터프라이즈 메시징 시스템](https://en.wikipedia.org/wiki/Enterprise_messaging_system)의 대안입니다. 하루에 1조 4천억 건의 메시지를 처리하기 위해 LinkedIn이 개발한 내부 시스템으로 시작했습니다. 

> Apache Kafka와 Google Cloud Pub/Sub 같은 이벤트 스트리밍 시스템이 등장하기 전의 데이터 처리는 일반적으로 원시 데이터를 먼저 저장했다가 나중에 임의의 시간 간격으로 처리하는 주기적인 일괄 작업으로 다뤄져 왔습니다. 예를 들어 통신 회사에서는 하루, 한 주 또는 한 달이 지날 때까지 기다렸다가 수백만 건의 통화 기록을 분석하고 누적 요금을 계산할 수 있습니다.

> 일괄 처리의 한계 중 하나는 실시간이 아니라는 점입니다. 적시에 비즈니스 결정을 내리고 흥미로운 일이 발생할 경우 조치를 취하기 위해 데이터를 실시간으로 분석하고자 하는 조직은 점점 많아지고 있습니다. 앞서 언급한 통신 회사에서는 전반적인 고객 경험을 향상시킬 한 가지 방법으로 고객에게 실시간 요금을 알려주는 것이 도움이 될 수 있습니다. 여기서 이벤트 스트리밍의 필요성이 발생합니다.


### 이벤트 스트리밍
> 이벤트 스트리밍은 데이터의 시간적 가치를 포착하는 것은 물론 흥미로운 일이 발생할 때마다 조치를 취하는 푸시 기반 애플리케이션을 만들기 위해 이벤트가 생성되는 대로 이벤트의 무한 스트림을 지속해서 처리하는 프로세스입니다. 

이벤트 스트리밍의 예
- 고객 대면 웹 애플리케이션에서 생성되는 로그 파일을 지속적으로 분석
- 사용자가 전자상거래 웹사이트를 탐색할 때 고객 행동을 모니터링하고 그에 대응
- 소셜 네트워크에서 생성되는 클릭 스트림 데이터의 변화를 분석하여 고객 감정에 지속해서 영향을 미치는 것
- 사물 인터넷(IoT) 기기에서 생성되는 원격 분석 데이터를 수집하고 그에 대응하는 것
- 데이터 처리를 위한 실시간 스트리밍


### Kafka의 개념



 
### 참고
- [kafka](https://kafka.apache.org/documentation/#introduction)
- [Apache Kafka란 무엇인가요?](https://www.tibco.com/ko/reference-center/what-is-apache-kafka)
- [Apache Kafka란?](https://cloud.google.com/learn/what-is-apache-kafka?hl=ko)